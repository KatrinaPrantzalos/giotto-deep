{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: image data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. run benchmarks\n",
    " 5. visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "train_indices = []\n",
    "for i in range(10240):\n",
    "    train_indices.append(i)\n",
    "\n",
    "print(len(train_indices))\n",
    "dl_tr, dl_temp = dl.build_dataloader(batch_size=512, sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "print(len(dl_tr))\n",
    "\n",
    "test_indices = []\n",
    "for i in range(3072):\n",
    "    test_indices.append(i)\n",
    "\n",
    "dl_ts, dl_temp = dl.build_dataloader(batch_size=512, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "dl_val = dl_ts\n",
    "\n",
    "print(len(dl_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "model = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL EPOCHS  2\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 1.587334  [20/20]\n",
      "Time taken for this epoch: 5s\n",
      "Validation results: \n",
      " Accuracy: 3.3%,                 Avg loss: 0.000161 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Training loss: 1.308019  [20/20]\n",
      "Time taken for this epoch: 5s\n",
      "Validation results: \n",
      " Accuracy: 4.0%,                 Avg loss: 0.000122 \n",
      "\n",
      "Test results: \n",
      " Accuracy: 4.0%,                 Avg loss: 0.000122 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from gdeep.search import gridsearch\n",
    "\n",
    "# print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "# pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer, True, \"loss\", 5)\n",
    "pipe = Pipeline(model, [dl_tr, dl_val, dl_ts], loss_fn, writer)\n",
    "# pipe = Pipeline(model, [dl_tr, dl_ts], loss_fn, writer)\n",
    "\n",
    "# search = gridsearch.Gridsearch(pipe, \"loss\", 5)\n",
    "# search.search([SGD, Adam], 1, lr=[0.001, 0.01])\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 2, cross_validation = True, batch_size = 512, lr=0.01)\n",
    "# pipe.train([SGD, Adam], 1, lr=[0.001, 0.01])\n",
    "# pipe.train([SGD, Adam], 1, lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-28 14:37:43,443]\u001b[0m A new study created in memory with name: no-name-b146e5c5-68eb-48a0-b3dc-dd2ea21c0098\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 4.345394  [20/20]]\n",
      "Time taken for this epoch: 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-28 14:37:49,383]\u001b[0m Trial 0 finished with value: 4.345393657684326 and parameters: {'optimizer': <class 'torch.optim.adam.Adam'>, 'lr': 0.003496633883137287}. Best is trial 0 with value: 4.345393657684326.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 2.2%,                 Avg loss: 0.000315 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  4.345393657684326\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.adam.Adam'>\n",
      "    lr: 0.003496633883137287\n"
     ]
    }
   ],
   "source": [
    "from gdeep.search.gridsearch import Gridsearch\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pipe = Pipeline(model, [dl_tr, dl_val, dl_ts], loss_fn, writer)\n",
    "# pipe = Pipeline([model1, model2, ...], [[dl_tr, dl_val, dl_ts], [dl_tr2, dl_val2, dl_ts2], ...], loss_fn, writer)\n",
    "\n",
    "search = Gridsearch(pipe, \"loss\", 1)\n",
    "search.start([SGD, Adam], 1, lr=[0.001, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch with multiple pipelines (models/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gdeep.search.gridsearch import Gridsearch\n",
    "\n",
    "# pipe1 = Pipeline(model1, [dl_tr, dl_ts, dl_ts], loss_fn, writer)\n",
    "# pipe2 = Pipeline(model2, [dl_tr, dl_ts, dl_ts], loss_fn, writer)\n",
    "\n",
    "# search1 = Gridsearch(pipe1, \"loss\", 2)\n",
    "# search1.search([SGD, Adam], 1, lr=[0.001, 0.01])\n",
    "\n",
    "# search2 = Gridsearch(pipe1, \"loss\", 2)\n",
    "# search2.search([SGD, Adam], 1, lr=[0.001, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking a single model on multiple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR10_10000\n"
     ]
    }
   ],
   "source": [
    "dataloaders_dicts = []\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "train_indices = []\n",
    "for i in range(5120):\n",
    "    train_indices.append(i)\n",
    "\n",
    "dl_tr, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "test_indices = []\n",
    "for i in range(1024):\n",
    "    test_indices.append(i)\n",
    "\n",
    "dl_ts, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"CIFAR10_5000\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr, dl_ts)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n",
    "\n",
    "\n",
    "train_indices = []\n",
    "for i in range(10240):\n",
    "    train_indices.append(i)\n",
    "\n",
    "dl_tr, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "test_indices = []\n",
    "for i in range(2048):\n",
    "    test_indices.append(i)\n",
    "\n",
    "dl_ts, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"CIFAR10_10000\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr, dl_ts)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n",
    "\n",
    "print(dataloaders_dicts[1][\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gdeep.pipeline import benchmark\n",
    "\n",
    "# bench = benchmark.Benchmark(writer)\n",
    "\n",
    "# bench.benchmark_model(model, dataloaders_dicts, loss_fn, SGD, 1, 0.01)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking a single dataset on multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dicts = []\n",
    "\n",
    "model = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"resnet18\"\n",
    "temp_dict[\"model\"] = model\n",
    "\n",
    "models_dicts.append(temp_dict)\n",
    "\n",
    "model = nn.Sequential(models.vgg16(pretrained=True), nn.Linear(1000,10))\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"vgg16\"\n",
    "temp_dict[\"model\"] = model\n",
    "\n",
    "models_dicts.append(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bench = benchmark.Benchmark(writer)\n",
    "\n",
    "# bench.benchmark_data(model_dicts, (dl_tr, dl_ts), loss_fn, SGD, 1, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gdeep.search.benchmark import Benchmark\n",
    "# from torch.optim import SGD, Adam, RMSprop\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# bench = Benchmark(writer)\n",
    "\n",
    "# bench.benchmark(models_dicts, dataloaders_dicts, loss_fn, optimizer = SGD, epochs = 1, learning_rate = 0.01, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking + Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-28 14:37:58,742]\u001b[0m A new study created in memory with name: no-name-688fd14d-2d65-4bc4-a123-0c5b91c2c083\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_5000, Model: resnet18\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 2.818259  [ 3/ 4]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-28 14:37:59,958]\u001b[0m Trial 0 finished with value: 2.6184284687042236 and parameters: {'optimizer': <class 'torch.optim.sgd.SGD'>, 'lr': 0.004082089833978034}. Best is trial 0 with value: 2.6184284687042236.\u001b[0m\n",
      "\u001b[32m[I 2021-07-28 14:37:59,959]\u001b[0m A new study created in memory with name: no-name-3cbc17c2-7a0f-471e-a617-ad1791da6dc1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.618428  [ 4/ 4]\n",
      "Time taken for this epoch: 1s\n",
      "Validation results: \n",
      " Accuracy: 0.2%,                 Avg loss: 0.000051 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  2.6184284687042236\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.sgd.SGD'>\n",
      "    lr: 0.004082089833978034\n",
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_5000, Model: vgg16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 2.614710  [ 4/ 4]\n",
      "Time taken for this epoch: 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-28 14:38:02,127]\u001b[0m Trial 0 finished with value: 2.6147100925445557 and parameters: {'optimizer': <class 'torch.optim.sgd.SGD'>, 'lr': 0.0011836768807636912}. Best is trial 0 with value: 2.6147100925445557.\u001b[0m\n",
      "\u001b[32m[I 2021-07-28 14:38:02,128]\u001b[0m A new study created in memory with name: no-name-4eed753a-e77f-4cc6-97bf-ee75b4653249\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.2%,                 Avg loss: 0.000050 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  2.6147100925445557\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.sgd.SGD'>\n",
      "    lr: 0.0011836768807636912\n",
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_10000, Model: resnet18\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 1.977874  [ 8/ 8]\n",
      "Time taken for this epoch: 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-28 14:38:04,504]\u001b[0m Trial 0 finished with value: 1.9778743982315063 and parameters: {'optimizer': <class 'torch.optim.sgd.SGD'>, 'lr': 0.006186748398487935}. Best is trial 0 with value: 1.9778743982315063.\u001b[0m\n",
      "\u001b[32m[I 2021-07-28 14:38:04,506]\u001b[0m A new study created in memory with name: no-name-9b827b3a-1e71-4dd9-a0a2-723be132e835\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.7%,                 Avg loss: 0.000078 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  1.9778743982315063\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.sgd.SGD'>\n",
      "    lr: 0.006186748398487935\n",
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_10000, Model: vgg16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 2.298507  [ 8/ 8]\n",
      "Time taken for this epoch: 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-28 14:38:08,733]\u001b[0m Trial 0 finished with value: 2.298506736755371 and parameters: {'optimizer': <class 'torch.optim.adam.Adam'>, 'lr': 0.0013277014946581002}. Best is trial 0 with value: 2.298506736755371.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.2%,                 Avg loss: 0.000097 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  2.298506736755371\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.adam.Adam'>\n",
      "    lr: 0.0013277014946581002\n"
     ]
    }
   ],
   "source": [
    "from gdeep.search.benchmark import Benchmark\n",
    "from gdeep.search.gridsearch import Gridsearch\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "bench = Benchmark(models_dicts, dataloaders_dicts, loss_fn, writer)\n",
    "\n",
    "search = Gridsearch(bench, \"loss\", 1)\n",
    "search.start([SGD, Adam], 1, 512, lr=[0.001, 0.01])\n",
    "\n",
    "# bench.benchmark(optimizer = [SGD,Adam], epochs = 1, learning_rate = [0.001,0.01], batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
