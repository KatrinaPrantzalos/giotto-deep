{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: image data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. run benchmarks\n",
    " 5. visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "\n",
    "# use only 320 images from cifar10\n",
    "train_indices = list(range(32*10))\n",
    "dl_tr, dl_ts = dl.build_dataloaders(batch_size=32, sampler=SubsetRandomSampler(train_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "# wrap a sequential model in a torch nn.Module\n",
    "class model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model3, self).__init__()\n",
    "        self.seqmodel = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))\n",
    "    def forward(self, X):\n",
    "        return self.seqmodel(X)\n",
    "\n",
    "model = model3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  3.290145993232727  \tBatch training accuracy:  18.359375  \t[ 8 / 8 ]                               \n",
      "Time taken for this epoch: 4.00s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 15.625000%,                 Avg loss: 3.360548 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  2.331341952085495  \tBatch training accuracy:  30.46875  \t[ 8 / 8 ]                               \n",
      "Time taken for this epoch: 3.00s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 17.187500%,                 Avg loss: 2.594628 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6170313954353333  \tBatch training accuracy:  47.65625  \t[ 8 / 8 ]                               \n",
      "Time taken for this epoch: 4.00s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 10.937500%,                 Avg loss: 4.567349 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.567349195480347, 10.9375)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 3, False, {\"lr\":0.01}, {\"batch_size\":32, \"sampler\":SubsetRandomSampler(train_indices)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply use interpretability tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/captum/_utils/gradient.py:56: UserWarning:\n",
      "\n",
      "Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "\n",
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/captum/attr/_core/guided_backprop_deconvnet.py:59: UserWarning:\n",
      "\n",
      "Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
      "\n",
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning:\n",
      "\n",
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAADQCAYAAAAK/RswAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmrUlEQVR4nO3daYxd933e8d//rrNzOBzupERSkrVZtqHYlpfalmM3dZy2jos4dlOkeRGgdYG2r/LCqIM2QYGizYskaNw0XeAirYsWKAo3Smw3dooUiWJZiy1TCylZssRF3Iecfblzl9MXHLm68zx3eDgz5EyOvx+Y4PjHc8/5n3PvPfevO8/5nZRlWQAAABRJaasHAAAAsNmY4AAAgMJhggMAAAqHCQ4AACgcJjgAAKBwmOAAAIDCqaz1jyM7RrPde/avqupl5SnpPKlUSlLLzHzKXaSeQh+bzIK61BprTW48uR4ZyV5Kb9ZnFrMX4fce+I1XuAEbXZsfTr615j3WbiOnf3hiIsuy3bk2tMnGx8ezI0eObMWmgYiIOHXqVExMTOQ9a2yqsbGx7ODBg+t6bDLn3Nvx2K1U9LYrW7V/L7zwwro+A9ac4Ozesz/+5W9/uavW6XRkuf56XWq1vj6pdcq6XCvTSU8lylIrt3V8VR3KdeZJyCq6naaZNbmnr9R2M5eqlFpNXa5dMgPP+d51Lyb7AjPr63TMWNyk7Ca27Z77dtvsn1ufqbXs/uk2fvlTbz+dayO3wJEjR+Lpp5/eqs0D8Z73vGfLtn3w4MH46le/2lVz54ZKRT9KXC3vxGWjy7kxumU3+wM79zl7k7exkeVu5vGbvX9513fPPfes6zOAX1EBAIDCYYIDAAAKhwkOAAAonDUzOBERnVW/tqzUNXuy3NEcxvz0rNSqg/o70HK1Xzea6XIdkx9pueRxRLSXmlJbml6UWq1PM0Ht0AzI3OKc1EpJHzs0uENqmVlfx+RW7O+HpdLjd8tmQZfBccew169PXd7GbdtlcNy+dMzedHLmfABsH+WyZiTd+3ZpaUlq1ap+frj1Ofa80uN84c5LrVZLai4n5M5zy8vLUiuV9PuBWq1mx5NnG5tto1mdvI93z0HevNOtzizxDQ4AACgcJjgAAKBwmOAAAIDCYYIDAAAKZ82QcbvTjpn57oBts6kB3okrV6X2xrnLUiv3DUptaHin1OolDfCa3HEst3QsERGdpobJFmY1KNxf1e1ESQNTs8samF5e1gEdO3qP1O6+607drmuCaIJaNkBnjkNmih2XPHalDQbMHBcwK7kxmgA2gO0jyzIJ2LoA78LCgtRmZ/W86UK9ddMo1gV4nV4hY1d3QWE3Hsc91m1jbGwsVy1vuLlInZHzNm8kZAwAALAGJjgAAKBwmOAAAIDCYYIDAAAKZ82E1dz8fHz7O0+sqpmuvqHdKRcbGhRaamsYuVrTWrmj8662ySctZRomvr6sbnuwpsHe/qS731fXrprtkgbM5uc14PzMc89K7fLEeakdO3pUauPj4zq+gQGpZe4u4Sb01zF35U7muPZsZbwBmeuCnLOzJZ2Mge1jeXk5zpw5I7XVXCjYnpfM+ztv0Nfpdb7Ie8dz10XZ1dz63AU3Fy5ckNrcnH5muuDxgDnfu87Pm90ReCuDzHQyBgAAuElMcAAAQOEwwQEAAIXDBAcAABTO2p2M252YmlvsqmWmpXAyLXIrNQ1HDZhQb7mktVroLeeXQgNrrR7zs9mFeaktzmutnjRMNpRpV82yOUrVer+OcW5Jaj88e05qpy9clNroyA6pHT50SGq7x3fpY3dqN+hKyYTnTPD4ZgJdbbNox3Qo9sEx06nZhoyL07kT+Muu0+lIqDjvxQEueOwCvG45V3Pb6NUdt9FoSM2Fnt12XLDXbcftS6ulF75MT09LzXV5dh2dd+zQzwUXRu7v18+jvJ2Db8ZGgstbEWbmGxwAAFA4THAAAEDhMMEBAACFwwQHAAAUzpoh406WxeJyd7CrWnUPMUHTtnZ5zEJrqazBL9OIOJabGuBt9hj98MCQ1GZnFqQ2s7wotYYJstVqGnoerukgy2Vdbr6lYTfXqbkxoUG0qSntgDk4pGGy/fsPSO2uo8ekNlTTEFvd7FuE79LZNE1Ds9Cgneui7ENnuj4XZAawdVaHc1241snblTxv+NSdk3px52zXgdmt043b7bMLI7vQct5xLyzoZ5QLS7vtDg8PS811S87bzTnCHwdCxgAAAFuMCQ4AACgcJjgAAKBwmOAAAIDCuXHIuNEd7m00dU7kOib29fVJzUWMTGPk6JiUsavNz2sINyKir19XWq9qkKrd1OWWGho8biUTtjLjqZnuwX4KaTo/V/SxbhuzC7rP06+clNrE1QmpDfdpV8xDB7VbckTETtMduWa6N7uAecd082yZvKHrRN3ONHQOYGtkWSbdeV1HYPcZ4AKtm80FhyPyB4DdvrhuxHkDsm4beTsKu8c6bp8nJvR870LLLnztuiVH+M/wvM/pRgLFmxlG5hscAABQOExwAABA4TDBAQAAhcMEBwAAFM6aiaEsy2J5VVfa1Na0qOt42CnlvFV73XRBLuu8q1PS4FevvFPTdCiuVTQwNdSvgauFZe2Y3ArddsPkoBotLdZLpnOk6f6bmblms2PCuqGhOBdOu3jtstTON65K7dXTZ6QWEbF797jUDhw4LLWhIe2g2Vc3AXMTwG5m+UJ/ALbO6tCnC4Hm7VrsuOCqC+a67fYK8LqgsNuOCyPnDRm7c5WrufNz3kBx3m7C7jjMz89LbW5OL1KZmpqy2x4YGJDayMiI1Op17ZC/kYD5Rl5Lq/ENDgAAKBwmOAAAoHCY4AAAgMJhggMAAApn7ZBxRLSyGwd+2iYMuzQ3qxszwaO2yYhVStqp0XU8rlZ9wKzidssFl0yn4KGaCZ2ZaWDH1JpmG6227ksp6YMz0+q3bQLF7bJJN5tcrmsGmZLZt6Z/fmfOT0rt9IVTUqvXNFDswmmuK2bddNV0ob+tNL88H0+de6qrVjHB8QPDB6S2v6PHwXm9c01qX372y1L7hYd+QWr3jd+XaxvAeiy3luPs1Nmumjt/DdWGpDa4qO8TF66dLDeldvzScak9tPchqY0P6MUQvbbjbHYHZnvBTc7QrAsPb2ZX315cqDrCd0yenp6WWt7wtluuXNaLT1xtvfgGBwAAFA4THAAAUDhMcAAAQOEwwQEAAIXDBAcAABTODW/V0Gh2J6ld6rzTyZf+bjX0FgqLjQWpVc2VTGWT3K9X/BU3WdLUeso0mW3bYHf0kiSze7HQ1uT5cuj6SuYWBcvmGFbNZWJZSdfXLOn4XNC+5JLoSW9D0etiA5fd75hLx5YXtfX3zLy5rMtcTRYNfWyv1utbpZRKMVDtvhpqz+AeWW5vZq6YunxRSrN37JPaHdU7pPbPT+zV9V36vpSyz3EVFW6t0qr/Dh6q6hVTAx39KOnM6pW0SyP62FpHr6Z8/xvm9g3XTkut9c5RqUX4q6jy3v4hb83dlsF9prjtupob80aurMq73Y2ec91xWFzUz3o7J+hxBddm4RscAABQOExwAABA4TDBAQAAhcMEBwAAFM6aIeNOpxMLS93B1IpLpZqAmbs1wuL8JanVaho8Gtt7SGr9JrdaMkHfiIhyv4bWspK2A5+evKpjnJuR2p1H75XabHNQapOT2sa6XtfwabOpgdtk7rfQcWEys8tuubZ5aC30GJTK/hi2mho8a7v7U7jbTjTmdYyr2r1HRFw995quL9tec+7+an88tOft3cWLGh6OmfNaM63Oh69psDq+/U0pvfK3/5rUXpvU4/VTujZg01RKlRir7eyqpQW9MKSzrO/5zARIW9em9LHnzklt/p1vk9p0Q8+vd/UIyOZt9+/CsI1GQ2q7du2SmgsUu/W52xa4YK6TN1C82cvdzLIupOzCw0tLepHLrAmib6bt9WkCAACwCZjgAACAwmGCAwAACocJDgAAKJy1OxlHFu3VYSGTO9pZ75fayKCGaxcHzOaSBjGrcxrU6mvpXGzPHu0oGxGx1N8nteWWBmz7+3SM5QHdl4GREamNDu6X2r5xDae5INqSCW8tmOUuXtFQdnN+SmrVTPet0tJAV7mjx7rZ9CGvSlmPTSf0uHZK5jld1HXOnD8ltcak7t/cnB7DLbW4GPHCC90183qIAwe01qfHK156SUp/+vCY1GaunJDap/7Rl6SWfZOYMW6drNmMzqVV79N6XZarj45KrTag55DqhQtSe/0uDfA2ZjXIf///fkZq/f/g7VKL8MFXF+ytm33pM+9bt5yrDQ1pp2YX1nWfC642N6cXJSybixecvNvt1U24UllzevAjrgOzW6fbFxc8zrt/efANDgAAKBwmOAAAoHCY4AAAgMJhggMAAApn7RRRlkW0ugM/OwaGZbFREx4+d+GM1BZrGspqmG7E6eJpqR3dpYHiPYcPSi0i4qXz2lU262jobGBew8w7BjVg9vzZ41Ib2qedO4fq2rHy9R9oWLQ9uFNqo/e8Q9d34G6pzZ8+KbWy6b48kmmga2FuSmuzl6UWEVGralhuZkm7g/aP7pbarn491nOmi3KYJqTJdcoODcbdLll/XzQfuK+rNrEwIcvtN+FC1807JvSxl5M+B5996HP62J//+d4DBW6FSiVau7pD8O2ShnWHB7Wz+8yMnpfStWtSu9Knyx37za9IbemRR6Q2Ys8XEZOTk1JzoVtXq9W0E/450215eFg/C10wd8K8591yY2N6sYELLU9NTUmt2dTzqwtauwCv677ca4wuqO1C2a57c15u3OvFNzgAAKBwmOAAAIDCYYIDAAAKhwkOAAAonBuGjEvt7vDSPhN6ujSpIcnmsAaFKiaUVUoaXG01NSB258MPSm2yR/h0eafpUJx0V0sjGo6amtEuvLNLGsLqLExJrbGkgekdZhtnTUfH+StXpXan6Q564F4NI0+d0G6Q8+c0qD15SWsz87rdiIi26Rw9vajPaf9ODRkPH9Zaa0GDhEuL2rW4VNLXw1Zqd9oxtTTVVdvfNy7LPXnuKam9+8C7pfbaOw5JbebUq7rhX/kVKf39D+sx/D19JLBpOlknFpvd578Du/bJcq9c0tfw3sG9Ups7rO+d0pSeD2sf+YjUnvqgXmjy0yb0GhFRLut5xHXczRvEdSFe14XXrc+FlhsNPfe5Tr8ueLxrl3Z+vnpVz+NufXm7CffiOhS7kPGOHTuk5o6hWx8hYwAAgDUwwQEAAIXDBAcAABQOExwAAFA4a4aMK+VyjI10B4PHhzQoPHXtktTG+rSTYb2q4aFWU0NGe+66V2rH9h+W2otnXpNaRMRoXUNdraYGx/bsG5VaaVxD1PMVnQeWhnUbk1cuSu3OPRoqXajpWCbb2hn52uQV3e7+O6R26IH3Se3cGy9JbWlxQWrVsg90ZW3t8FnuaEisMaUB8yuhQe3Wgm67VNbj2iMzuGVanVZcW+zuvrr7V/+FLPfI44/rgy/rsbnnK9qh9Z7feEwf+3M/J6V/966flJo+S8Dmysrdr7K+P/9zWebQce32XjIdcoc/+1mp7Xn8h1KrvetdUvs7975XaldNSDXCB3tdF96BAb0gpb+/X2p5Ox67EO/IyIjUXOC5YzqfL5jzpuugvGePBrDdWFzQ142l13jcccgbts4bKHbbWC++wQEAAIXDBAcAABQOExwAAFA4THAAAEDhrBkyrlXLcee+7k6Kf+unNeh4+rUjUptd0oBTY0nDta2GBo+OHNAgbdYxt7of146aERHTJlA8v6DjOTSuwaxWpsGquXkNUWV9dakNZTulVu5osG3vDg2xzV/WQPHcOQ2YNRs6vsG9GmQ+8OCHpNZpTkvt8nkN+EVELMxpUDjMvowMasfQSmi4MDOvtOaCri+LzetiuRkqpUqM9Xe/B/70H/9NWe6jv/Zr+uCvf11r0/ocxN13S+n5TzwstYdeMqH6Q/rcA5ulUqnE7lXdymu/eL8s97bPfEZqzRMnpNY2HYbb9+pFJY2ffERq9St6Ptwxrp2RI3yY1nUodgFgF65163PdkvNy3X9nZ00XfVNz43Odg/fu1U7SLug7M6Md0iP8PrsAcKWiJ3e3nAsUu+D3ZuIbHAAAUDhMcAAAQOEwwQEAAIXDBAcAABTOmiHjcspipNwdsH3/wxoAfu+DB6U2u6C3g29mOp9qtjSM1FrQkOrikq7v6LJuNyJioaHBpbl5XWe1qrs/aQJXfUe1Y+Wiud19NqqBt3MXL0jtldfPSO2BnRp4PnPlmtSiY0J6fdrZcuhODal+6K4jUrt21oeMX/7ed6V2+eLLUhtMk/rghnZlXmrruJMJy1WqutxSS8OBt0ulVInxge7n9dEjj8pytvfm5z6XbyM/8zNSertZLNOXCHBLlVIphurd3d3fd592TnfB1+X77su1DfdYF4Zt3aG1nT1Cqu7xebv4us68LlDstuFCyy4ofO2antuHhrSLvutG7Mbnui+Pjo5KbedOvRCmV8j44kXtzD9tLpJwgWIXHs7bobhXZ+X14BscAABQOExwAABA4TDBAQAAhcMEBwAAFM6aIeNOqxVz17pDpG+8/oIsd+jgUakd3K9dFCsDGobtJB3CzMSE1KamNMy6a2yX1CIi5hc1TLawaLobz2kYdnZOO0Lee9cxfey8CdIuapB5d792PK42dHw/8cgHpHZtQZc7dVFDXssl7YrZXtQgWqzqSBoRceAd+txFROx+x1+VWmvyko7x5JNSe/2Fp6U28cMfSK1U02NYqmjgMBpbFzIGfpxlWSahVheQdZ10h4f1fF+tVnNtd9GcS/OGayN6hJRNzXU3bpgLSHbt0s8a91i3jVpNL1JxwerDhw/n2oYLHrtgrttGva6fR/v2+TsCuE7I7thMmM/rS5fMZ4V53bguyK7j8XrxDQ4AACgcJjgAAKBwmOAAAIDCYYIDAAAKZ+1OxqVyjPYPdtVmr2p3wwsmzDS+T4NCO8q6ucHhUd3wDg2nlZMGbod9vix2DOnjs5IGvVpNDXCdPPGS1Hbv1nDuwIB2dF4woeV3HtFuyx95t3YZXjQdnRc0rxb3HNYOkZeuaiDv/EUNdF18/azUzrR9d8klEwjvHz0ktdG3f0Jq77r3/VI7+PpzUnvu21+X2pWLr5vR+E6bAG6tlJKEZBcWFnI91oVcBwcHpeaCr67muJBqhA/2uvCqG+OVK1ek5sbtAtMuFDw2Nia1Q4f0XOrG4mquQ7ELYLswsuta7ILREb57swt1u89HF8qenNQLhU6fPi011/l5vfgGBwAAFA4THAAAUDhMcAAAQOEwwQEAAIWzZsi4Wi7H/rHuDpVpWcO+1y5dltrx516V2rMvvCy1vQe1e+OHPvJhqR3crZ0ylyZ92K1cMeljEzJ2AbU7Dujt5Pv7NExWr+nccKQ2oNsd1u0227qNWdN9ebGtobiTr5yS2mRDQ3EPH9Pg19we3d/XL2hoPCLi5GkNWx9/TZ/T2fqo1MZH9Dg8sFfD1u/+sHZLfvaJb0ltZkpfXwBuvVKpFAMD3e9nF3x1nd1dN9vLl/W97DoeHzlyRGou6Os6Hkf4zr55a0NDQ1JzgWIXwnW1vj7tNO/2xYV93bG+evVqrsfu3KmfM267Lngc4UPBbtvuGLp9zvs8u+DxevENDgAAKBwmOAAAoHCY4AAAgMJhggMAAApnzZDx4sJ8PPfs01217KoGgHbs0kDrd1/UkOpLJiD7wY9+TGpf+a//RWp/42N/RWo7+3wX3r5+DTNVqhp8XVzSkPLuXXuk1qlrMGvS3DbeSWWdQzbNvDJVNZT16uk3pPZbv/lbUpu4rF2LH3mfHq+//plflNqeffrcRUQMtjS8d6CloecXp0z3zZIG3i6f0dfNPXfsldqxex+Q2g+ef9KOEcCt1Ww24/z58101F+xdHUSOiLh4US9guHZNz1VHjx6V2vHjx6V21113Sa1XJ+O8oWAXznX74tbXbmtXecd1UM4y/exy43NB38cff1xqrrv0HXdot/0HH3xQai543IsLYLuAudtnF2besUMvHnKdkdeLb3AAAEDhMMEBAACFwwQHAAAUDhMcAABQOGuGjJvtTlyZ6g4vvVTVrrnly9rd8MyFC1L78Mceldo/+dUvSu13vvS7UvvaHz4mtfsO6i3ZIyKqNQ1rDQ6PSM2FxMZ26K3td49pGNaF22o17VpcSrrcXFuDbcsVnWv+29/7T1I78dLzUqtXdbtffex/SO3QvQ9J7aF73ia1iIj+uoaeRzId9wHNnEXL7Mu86cqcLWtQ+86DGowDsDU6nU4sLS111aanp2U5FyB1NRcofvTRR6X25JN6YcFLL+mFKy6kGuEDu+787MK+eTsPuw6+brsucJs3oPzUU09JzXWDdtt98cUXpbZrl35mjo+P2227zzjXWbler0vN7bPj1tfrOV0PvsEBAACFwwQHAAAUDhMcAABQOExwAABA4awZMq7V63HwyN1dtXbMynLN5pLUaoOaPt1/+KDUsqQhr8MHDkntT/7gf0pt9qLeDj4iYqBfQ0/1/n6zpAah6hXtWDk0oPsy0K/dLmsm7NtX0+1mfTq+K4t6XF88eUJqH/+4dn5+57veKbX/8B81oPzEn31Dasf2jUotIqI2oKG1CdOZ9PgrP5BadVD3ee+Ibqe9qEG7/hpzbmC7KJfLMTo62lVzwVAXmnXdf0dG9GIPxy3nQsZzc3P28W7brua4wG7e9bnHurCuW251mDvCB4rvvvtuqe3bt09qzzzzjNRee+01qQ0Pa+f/XmN0HZMvXbokNbfPrkN03mOzXnyaAACAwmGCAwAACocJDgAAKBwmOAAAoHDWDBlnkUUrusNj7Y6Ggmt1DQ8NmizZzJwJKF3WzsgT1/QW8W9c1G7JWaupG4mIvrqGXJtNDcHpnkTUq3pIBusmTFbRIFS/6YDZ16fHplPWcPOZKxrUikyX+9lPf1pqH/jAB6R29uwbUvvqY38otWeP36nbjYj20rLUJi9pB9Plq+ekVmlraG2hpWHA1ybPSm2grkFtAFtndbdfFzJ2YVHX4XZ5Wc8rLijswqyzs3ohhhtLr/H0WnY116HYrS/vcq7mOv26/XPuv/9+qd1xh3aAdx2nT548KbVz5/QcHuGD4+65WlxclJp77h33PLvjtV58gwMAAAqHCQ4AACgcJjgAAKBwmOAAAIDCWTPN02q1Y2KqO9zbbGm3xYoJW2UtDSg9+9wLUnvonT9hlnteak0zF1uuuO7EEctNDQBfuDAhtaWG6cBsAk5V01jR3Qy+WnPdLnV97UzDbnNLGtQaG98rtXFzu/vZmRmp7duvnS2vTWqg+5vf/LrUIiKW5ualdvWqBszmkwnamU7SZROY3rl3t9T27NVxA9ganU5HQqQufOpCs6vDyRERF003dNeF1y3nuO1G5A/ItlotqbnwsKs5bjnXmdcdm2ZTL5px3X9drdFoSM11KHaB4FdeeUVqvcbjHu/2xe2ze67cvgwN6Z0D1otvcAAAQOEwwQEAAIXDBAcAABQOExwAAFA4THAAAEDhrH2rhpRFO3Vf8ZPK2kp/zrRbXjSJ9YtX9HYLv/07X5La6VdP6zaWNRX/6jm9KigiIjO3k3Cp+mZbr2ZKbU2jl808MJnrqNKiuR1E0pS+zf2bJHr/oI7l6lU9hvWaPicz03plVaOhYzl1Sm/pEBGRzFVwTdPpPDO3onC3wKhVdYyDdU3LL8zrdgFsndVXybgrhdwVN+62DPPzenXmE088IbWpqSmpuSueZswVpBH+yh53qwZXy3uVmJP3arK83D672xu4q5aWlvQqYbdv7lhH+HG72kauMKtW9cpjt8/rxTc4AACgcJjgAACAwmGCAwAACocJDgAAKJw1Q8aVSiXGdo2tqmqYadG09W8MaoC0ZNr6T01OSW3X7j1S2zGmbf1bJkwcEdHJNNzWampgt23CTM2mhrA6zXyh5UZDt9txATNzq4aSmWtOmQDdX3z7L6T20Y9+VGovnjgpNTPkWO5xDMvmee6Y588FtdsNDRzGsm7n7Omzut26thcHsDVKpVL093ffEscFaV3IuGYufnBcGNa18F89jojeAd684WH3+LzLbWQbebljc+bMGakdO3ZMapcvX841FrcfEf55zhuiznscpqenpVYxt0taL77BAQAAhcMEBwAAFA4THAAAUDhMcAAAQOGs3ck4smhHdwDJBZIqdQ2T1esaEnPhoZ07x3XDpotux4RhS6Z7Y0REa1k7PXbaGgBum4Cs2z+XEWs1NaA8N6/dmxsNDTc3m2YsZp/dY//oa1+T2gsnTkjtme9+T2qppF0j276vcrRccMyEo7OWOYZtPTauN2WppM9fX2YCygC2DXeOdF1q+/r6pOZCqjcTHs6zvgjfDTdvwDbvtt1jXfdmN5a8IVz32JdffllqLlB87tw5qeXtyNxrPBs5No4bT97H5sE3OAAAoHCY4AAAgMJhggMAAAqHCQ4AACicNUPGKVKk1B0ErVZ1TpTKJrjU1pq7NXq4Rr8meFR3geIegama2asUGnhzQeG2Czi5W8Sb8ewaX931OaJptpGZsK4PPGsQbX5eA9QXL12S2pEjR6U2O68B3oXFRamtjFIquYPHLoRojpcLJpZK+pwuzFztMUYAt5t73+aVt0tt3iBzL3mX3Ujn4byB6bxB5rw1F2Sem9MLXHbu3Ck113Ha1XrZyLjdc3Izoef14BscAABQOExwAABA4TDBAQAAhcMEBwAAFM4NOhmnyLLucGjWMbdQN91wXXbIha1s8LiigVQXRir1CiiZx5dNwKlquiO7wJULorkGwJlZXznp/rVMp1+Xoa6aMfcPj0rt4B3aSdp1fl5c1v1wIejrj9fnKpV1PHm7g5bNDrrj6ro3nzv9uh0jgFtvM4OgecOn7nxxM+PIG2h1oWd3/srbXdft30a69brH1mp6vnf74cbizrn2863H4914NhLKztu9eb34BgcAABQOExwAAFA4THAAAEDhMMEBAACFs3bIuJPF8lJ3AMkFhUz21AZkbfjUhKOSCQlnprNux7VBjoiUTIdcE/at9mstK2vIuO520G9Z15czRNU03Sk7pkuwe+zCsuuMrMGxpZbuW8/gnulOnbmun+Y5zRuCcwYGBnItB+DWy7JMzjnunGEvAjGfAe68dLs63OYNGbsxbvZ43Gdh3q7Kbjn3uZD3sTcjb9diFxLP+3qwFx6tE9/gAACAwmGCAwAACocJDgAAKBwmOAAAoHBumPzMstXhKg1btVuu06/W6vW61HznYK1Vaxo86tUNshK6bNt07G2ZjLLtzGvCzKVSvqBdch2U66bTclWDuW59LpTljkPTBIpLHT0GnR6hs5apl+W1ENHJGW7L2+3SBdEAbG/uHJQ3fJo3XJu3W29E/nNn3nNV3vNS3gB23hBu3i7IeTvK5w0391pn3uU28hmwmYFuPk0AAEDhMMEBAACFwwQHAAAUDhMcAABQOGmt4E9K6UpEnL59wwGsO7Ms270VG+Y9gG2A1z9+3K3rPbDmBAcAAOAvI35FBQAACocJDgAAKBwmOAAAoHCY4AAAgMJhggMAAArnhveiAn6cfSKlbCIiIqXrf9b7841qN7OOWzWeW7WOiMgi+9G9aN78OVu5x9vqn99c5vr/svw/r7Gujfx8o/X2Wmaz9inOxx9nWfaJ2AKfuPvubGJxsbv41nsFbdXP23Vda2zjR89n9paf11vPVi2zann7bzdbN9vOs66bWj7nfl/4wYV1vQeY4ABrmIiIZyqViDf/lErdf7vaWv+2Fctv4Rizcik6WSdandaP/n7rz2vVttvyWzXGzj/rjG/Z639hIZ75/Oevf1iXSv//77f+vFbtVi+/Fdu8yeU7KxPhTtaJLFb+Xvn/eWtv/v+8tZtd/1Zs82bW/+sf/fV1vQf4FRUAACgcJjgAAKBwmOAAAIDCYYIDAAAKhwkOAAAoHCY4AACgcLibOLCGlNILEbG01eNYw3hcv5p9u9ru44vY/mPsy7Ls7VuxYV7/m2K7j3G7jy9ine8B+uAAa1vKsuzdWz2IXlJKzzC+jdnuY0wpPbOFm+f1v0HbfYzbfXwR638P8CsqAABQOExwAABA4TDBAdb277d6ADfA+DZuu49xK8fHsdm47T7G7T6+iHWOkZAxAAAoHL7BAQAAhcMEB3iLlNJnUkovppQ6KaWeVxaklD6RUno5pfRqSukLt3F8Yymlb6WUXln5e2eP5U6llJ5PKX3/dlyFc6Pjka771yv//lxK6eFbPaabHN+jKaXpleP1/ZTSP73N4/tySunyymXZ7t9vy/Hj9b/ucW3r13/OMRbvPZBlGX/4w5+VPxFxf0TcGxH/NyLe3WOZckT8MCKORUQtIo5HxAO3aXy/ERFfWPn5CxHxr3osdyoixm/TmG54PCLikxHxjYhIEfG+iHjyNj6necb3aET80Ra+7j4cEQ9HxAs9/v22HD9e/7fs9bVlr/+bGGPh3gN8gwO8RZZlJ7Mse/kGi703Il7Nsuy1LMuWI+K/R8Snbv3oIla28/srP/9+RPzsbdruWvIcj09FxH/OrvtORIymlPZvo/FtqSzL/iwirq2xyG05frz+12W7v/7zjnFL3Yr3ABMc4OYdjIizb/n/b6zUboe9WZZdiIhY+XtPj+WyiPhmSum7KaW/d4vHlOd4bOUxy7vt96eUjqeUvpFSevD2DC23rTx+22ksvP7X58fyPUAnY/zYSSn9SUTsM//0xSzL/iDPKkxt0y5HXGt8N7GaD2ZZdj6ltCcivpVSemnlv5BuhTzH45YesxvIs+3vRcSdWZbNpZQ+GRH/KyLuudUDuwmbdvx4/W+67f76z7v9wr0HmODgx06WZR/f4CreiIjDb/n/hyLi/AbX+SNrjS+ldCmltD/LsgsrX89e7rGO8yt/X04pfTWuf0V9q07weY7HLT1mN3DDbWdZNvOWn7+eUvrdlNJ4lmXb5R49m3b8eP1vuu3++s+1/SK+B/gVFXDzno6Ie1JKR1NKtYj4XEQ8dpu2/VhE/NLKz78UEfJf3CmlwZTS8Js/R8RPRYS9MmGT5Dkej0XE3125EuJ9ETH95q8aboMbji+ltC+llFZ+fm9cPzdevU3jy2Mrj99qvP67bffXf64xFvI9sFWJaf7wZzv+iYhPx/X/UmhExKWI+OOV+oGI+PpblvtkRPwgrl+Z8MXbOL5dEfF/IuKVlb/HVo8vrl8pcXzlz4u3Y3zueETE5yPi8ys/p4j4Nyv//nz0uEJnC8f3D1eO1fGI+E5EfOA2j++/RcSFiGiuvP5+eSuOH6//Yr7+c46xcO8BOhkDAIDC4VdUAACgcJjgAACAwmGCAwAACocJDgAAKBwmOAAAoHCY4AAAgMJhggMAAAqHCQ4AACic/wdQsLXiUZrRBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "inter = Interpreter(pipe.model, method=\"GuidedGradCam\")\n",
    "output = inter.interpret_image(next(iter(dl_tr))[0][0].reshape(1,3,32,32), \n",
    "                      1, pipe.model.seqmodel[0].layer2[0].conv1);\n",
    "\n",
    "# visualise the interpreter\n",
    "vs = Visualiser(pipe)\n",
    "vs.plot_interpreter_image(inter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqmodel.0.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "seqmodel.0.bn1.weight torch.Size([64])\n",
      "seqmodel.0.bn1.bias torch.Size([64])\n",
      "seqmodel.0.bn1.running_mean torch.Size([64])\n",
      "seqmodel.0.bn1.running_var torch.Size([64])\n",
      "seqmodel.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.0.bn1.weight torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.bias torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.0.bn2.weight torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.bias torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.1.bn1.weight torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.bias torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.1.bn2.weight torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.bias torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "seqmodel.0.layer2.0.bn1.weight torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.bias torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "seqmodel.0.layer2.0.bn2.weight torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.bias torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "seqmodel.0.layer2.0.downsample.1.weight torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.bias torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "seqmodel.0.layer2.1.bn1.weight torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.bias torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "seqmodel.0.layer2.1.bn2.weight torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.bias torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "seqmodel.0.layer3.0.bn1.weight torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.bias torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "seqmodel.0.layer3.0.bn2.weight torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.bias torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "seqmodel.0.layer3.0.downsample.1.weight torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.bias torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "seqmodel.0.layer3.1.bn1.weight torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.bias torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "seqmodel.0.layer3.1.bn2.weight torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.bias torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "seqmodel.0.layer4.0.bn1.weight torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.bias torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "seqmodel.0.layer4.0.bn2.weight torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.bias torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "seqmodel.0.layer4.0.downsample.1.weight torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.bias torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "seqmodel.0.layer4.1.bn1.weight torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.bias torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "seqmodel.0.layer4.1.bn2.weight torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.bias torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.fc.weight torch.Size([1000, 512])\n",
      "seqmodel.0.fc.bias torch.Size([1000])\n",
      "seqmodel.1.weight torch.Size([10, 1000])\n",
      "seqmodel.1.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "me = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k,item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the decision boundary computations:\n",
      "Step: 0/1\r"
     ]
    }
   ],
   "source": [
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(dl_tr))[0]\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 7, 7])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 64, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 128, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([512, 256, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1000, 512])\n",
      "torch.Size([1000])\n",
      "torch.Size([10, 1000])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "x, target = next(iter(dl_tr))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vs.plot_data_model()\n",
    "# vs.plot_activations(x)\n",
    "vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "\n",
    "In the next section we compute the confusion matrix on the entire training dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 10.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11.25,\n",
       " 14.589171409606934,\n",
       " array([[25.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [31.,  0.,  1.,  4.,  0.,  0.,  4.,  4.,  1.,  2.],\n",
       "        [24.,  0.,  0.,  0.,  0.,  0.,  2.,  1.,  0.,  2.],\n",
       "        [25.,  0.,  0.,  2.,  0.,  0.,  1.,  1.,  1.,  2.],\n",
       "        [25.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
       "        [14.,  0.,  0.,  2.,  3.,  0.,  3.,  4.,  0.,  1.],\n",
       "        [28.,  0.,  1.,  0.,  1.,  0.,  3.,  1.,  1.,  2.],\n",
       "        [18.,  0.,  0.,  0.,  1.,  0.,  3.,  6.,  0.,  1.],\n",
       "        [27.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [31.,  0.,  1.,  1.,  0.,  0.,  4.,  2.,  1.,  0.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.evaluate_classification(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
