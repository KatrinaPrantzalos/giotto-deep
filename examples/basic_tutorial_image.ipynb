{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: image data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. run benchmarks\n",
    " 5. visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "\n",
    "# use only 320 images from cifar10\n",
    "train_indices = list(range(32*10))\n",
    "dl_tr, dl_ts = dl.build_dataloader(batch_size=32, sampler=SubsetRandomSampler(train_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "model = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Training loss:  2.72448468208313  [ 10 / 10 ]                      \n",
      "Time taken for this epoch: 4s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 1.4%,                 Avg loss: 0.049720 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Training loss:  1.87729012966156  [ 10 / 10 ]                      \n",
      "Time taken for this epoch: 4s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 1.3%,                 Avg loss: 0.058120 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.05812, 1.2617295353412628)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 2, False, {\"lr\":0.01}, {\"batch_size\":32, \"sampler\":SubsetRandomSampler(train_indices)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply use interpretability tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "inter = Interpreter(model, method=\"LayerGradCam\")\n",
    "output = inter.interpret_image(next(iter(dl_tr))[0][0].reshape(1,3,32,32), \n",
    "                      6 ,model[0].layer2[0].conv1);\n",
    "\n",
    "# visualise the interpreter\n",
    "vs = Visualiser(pipe)\n",
    "vs.plot_interpreter_image(inter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "0.bn1.weight torch.Size([64])\n",
      "0.bn1.bias torch.Size([64])\n",
      "0.bn1.running_mean torch.Size([64])\n",
      "0.bn1.running_var torch.Size([64])\n",
      "0.bn1.num_batches_tracked torch.Size([])\n",
      "0.layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "0.layer1.0.bn1.weight torch.Size([64])\n",
      "0.layer1.0.bn1.bias torch.Size([64])\n",
      "0.layer1.0.bn1.running_mean torch.Size([64])\n",
      "0.layer1.0.bn1.running_var torch.Size([64])\n",
      "0.layer1.0.bn1.num_batches_tracked torch.Size([])\n",
      "0.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "0.layer1.0.bn2.weight torch.Size([64])\n",
      "0.layer1.0.bn2.bias torch.Size([64])\n",
      "0.layer1.0.bn2.running_mean torch.Size([64])\n",
      "0.layer1.0.bn2.running_var torch.Size([64])\n",
      "0.layer1.0.bn2.num_batches_tracked torch.Size([])\n",
      "0.layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "0.layer1.1.bn1.weight torch.Size([64])\n",
      "0.layer1.1.bn1.bias torch.Size([64])\n",
      "0.layer1.1.bn1.running_mean torch.Size([64])\n",
      "0.layer1.1.bn1.running_var torch.Size([64])\n",
      "0.layer1.1.bn1.num_batches_tracked torch.Size([])\n",
      "0.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "0.layer1.1.bn2.weight torch.Size([64])\n",
      "0.layer1.1.bn2.bias torch.Size([64])\n",
      "0.layer1.1.bn2.running_mean torch.Size([64])\n",
      "0.layer1.1.bn2.running_var torch.Size([64])\n",
      "0.layer1.1.bn2.num_batches_tracked torch.Size([])\n",
      "0.layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "0.layer2.0.bn1.weight torch.Size([128])\n",
      "0.layer2.0.bn1.bias torch.Size([128])\n",
      "0.layer2.0.bn1.running_mean torch.Size([128])\n",
      "0.layer2.0.bn1.running_var torch.Size([128])\n",
      "0.layer2.0.bn1.num_batches_tracked torch.Size([])\n",
      "0.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "0.layer2.0.bn2.weight torch.Size([128])\n",
      "0.layer2.0.bn2.bias torch.Size([128])\n",
      "0.layer2.0.bn2.running_mean torch.Size([128])\n",
      "0.layer2.0.bn2.running_var torch.Size([128])\n",
      "0.layer2.0.bn2.num_batches_tracked torch.Size([])\n",
      "0.layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "0.layer2.0.downsample.1.weight torch.Size([128])\n",
      "0.layer2.0.downsample.1.bias torch.Size([128])\n",
      "0.layer2.0.downsample.1.running_mean torch.Size([128])\n",
      "0.layer2.0.downsample.1.running_var torch.Size([128])\n",
      "0.layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "0.layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "0.layer2.1.bn1.weight torch.Size([128])\n",
      "0.layer2.1.bn1.bias torch.Size([128])\n",
      "0.layer2.1.bn1.running_mean torch.Size([128])\n",
      "0.layer2.1.bn1.running_var torch.Size([128])\n",
      "0.layer2.1.bn1.num_batches_tracked torch.Size([])\n",
      "0.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "0.layer2.1.bn2.weight torch.Size([128])\n",
      "0.layer2.1.bn2.bias torch.Size([128])\n",
      "0.layer2.1.bn2.running_mean torch.Size([128])\n",
      "0.layer2.1.bn2.running_var torch.Size([128])\n",
      "0.layer2.1.bn2.num_batches_tracked torch.Size([])\n",
      "0.layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "0.layer3.0.bn1.weight torch.Size([256])\n",
      "0.layer3.0.bn1.bias torch.Size([256])\n",
      "0.layer3.0.bn1.running_mean torch.Size([256])\n",
      "0.layer3.0.bn1.running_var torch.Size([256])\n",
      "0.layer3.0.bn1.num_batches_tracked torch.Size([])\n",
      "0.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "0.layer3.0.bn2.weight torch.Size([256])\n",
      "0.layer3.0.bn2.bias torch.Size([256])\n",
      "0.layer3.0.bn2.running_mean torch.Size([256])\n",
      "0.layer3.0.bn2.running_var torch.Size([256])\n",
      "0.layer3.0.bn2.num_batches_tracked torch.Size([])\n",
      "0.layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "0.layer3.0.downsample.1.weight torch.Size([256])\n",
      "0.layer3.0.downsample.1.bias torch.Size([256])\n",
      "0.layer3.0.downsample.1.running_mean torch.Size([256])\n",
      "0.layer3.0.downsample.1.running_var torch.Size([256])\n",
      "0.layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "0.layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "0.layer3.1.bn1.weight torch.Size([256])\n",
      "0.layer3.1.bn1.bias torch.Size([256])\n",
      "0.layer3.1.bn1.running_mean torch.Size([256])\n",
      "0.layer3.1.bn1.running_var torch.Size([256])\n",
      "0.layer3.1.bn1.num_batches_tracked torch.Size([])\n",
      "0.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "0.layer3.1.bn2.weight torch.Size([256])\n",
      "0.layer3.1.bn2.bias torch.Size([256])\n",
      "0.layer3.1.bn2.running_mean torch.Size([256])\n",
      "0.layer3.1.bn2.running_var torch.Size([256])\n",
      "0.layer3.1.bn2.num_batches_tracked torch.Size([])\n",
      "0.layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "0.layer4.0.bn1.weight torch.Size([512])\n",
      "0.layer4.0.bn1.bias torch.Size([512])\n",
      "0.layer4.0.bn1.running_mean torch.Size([512])\n",
      "0.layer4.0.bn1.running_var torch.Size([512])\n",
      "0.layer4.0.bn1.num_batches_tracked torch.Size([])\n",
      "0.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "0.layer4.0.bn2.weight torch.Size([512])\n",
      "0.layer4.0.bn2.bias torch.Size([512])\n",
      "0.layer4.0.bn2.running_mean torch.Size([512])\n",
      "0.layer4.0.bn2.running_var torch.Size([512])\n",
      "0.layer4.0.bn2.num_batches_tracked torch.Size([])\n",
      "0.layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "0.layer4.0.downsample.1.weight torch.Size([512])\n",
      "0.layer4.0.downsample.1.bias torch.Size([512])\n",
      "0.layer4.0.downsample.1.running_mean torch.Size([512])\n",
      "0.layer4.0.downsample.1.running_var torch.Size([512])\n",
      "0.layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "0.layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "0.layer4.1.bn1.weight torch.Size([512])\n",
      "0.layer4.1.bn1.bias torch.Size([512])\n",
      "0.layer4.1.bn1.running_mean torch.Size([512])\n",
      "0.layer4.1.bn1.running_var torch.Size([512])\n",
      "0.layer4.1.bn1.num_batches_tracked torch.Size([])\n",
      "0.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "0.layer4.1.bn2.weight torch.Size([512])\n",
      "0.layer4.1.bn2.bias torch.Size([512])\n",
      "0.layer4.1.bn2.running_mean torch.Size([512])\n",
      "0.layer4.1.bn2.running_var torch.Size([512])\n",
      "0.layer4.1.bn2.num_batches_tracked torch.Size([])\n",
      "0.fc.weight torch.Size([1000, 512])\n",
      "0.fc.bias torch.Size([1000])\n",
      "1.weight torch.Size([10, 1000])\n",
      "1.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "me = ModelExtractor(model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k,item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the decision boundary computations:\n",
      "Step: 0/1\r"
     ]
    }
   ],
   "source": [
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(dl_tr))[0]\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 7, 7])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 64, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 128, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([512, 256, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1000, 512])\n",
      "torch.Size([1000])\n",
      "torch.Size([10, 1000])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "x, target = next(iter(dl_tr))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# vs.plot_data_model()\n",
    "# vs.plot_activations(x)\n",
    "vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
