{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toplogy of Deep Neural Networks\n",
    "\n",
    "This notebook will show you how easy it is to use gdeep to reproduce the experiments of the paper [Topology of Deep Neural Networks](https://arxiv.org/pdf/2004.06093.pdf), by Naizat et. al. In this work, the authors studied the evolution of the topology of a dataset as embedded in the successive layers of a Neural Network, trained for classification on this dataset.\n",
    "\n",
    "Their main findings can be summarized as follows:\n",
    "\n",
    "- Neural networks tend to simplify the topology of the dataset accross layers.\n",
    "\n",
    "- This decrease in topological complexity is more efficient when the activation functions are non-homeomorphic, as it is the case for ReLu or leakyReLu.\n",
    "\n",
    "Here is an illustration from the paper:\n",
    "\n",
    "![img](./images/topology_accross_layers.png)\n",
    "\n",
    "The main steps of this tutorial will be as follows:\n",
    "\n",
    "1. Create the Entangled Tori dataset.\n",
    "2. Build several fully connected networks, with different activation functions.\n",
    "3. Train these networks to classify the Entangled Tori datasets.\n",
    "4. Visualise in tensorboard the persistence diagrams of the dataset embedded in each layers of each network.\n",
    "5. Study the decrease in topological complexity of the dataset accross layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import autograd  \n",
    "\n",
    "#gdeep\n",
    "from gdeep.data.datasets import DatasetBuilder, DataLoaderBuilder\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualisation import persistence_diagrams_of_activations\n",
    "from gdeep.data.preprocessors import ToTensorImage\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.search import Benchmark\n",
    "from gdeep.search import GiottoSummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "# plot\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "writer = GiottoSummaryWriter()\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# TDA\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "#Tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Entangled Tori dataset and prepare the dataloaders\n",
    "\n",
    "![img](./images/entangled_tori.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  RandomSampler\n",
    "db = DatasetBuilder(name=\"EntangledTori\")\n",
    "ds_tr, ds_val, ds_ts = db.build( n_pts = 50)\n",
    "dl_tr, dl_val, dl_ts = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build(    \n",
    "     [{\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}, \n",
    "     {\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}, \n",
    "     {\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models with different activations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Choose the achitecture of the fully connected network\n",
    "architecture = [3,5,5,5,5,5,2]\n",
    "# Choose the loss function for training\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Choose the set of activation functions to equip the neural network with\n",
    "activation_string = [\"relu\", \"leakyrelu\", \"tanh\", \"sigmoid\"]\n",
    "activation_functions = [F.relu, F.leaky_relu, torch.tanh, torch.sigmoid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and trainers\n",
    "models = []\n",
    "writers = []\n",
    "trainers = []\n",
    "for i in range(len(activation_functions)):\n",
    "    model_temp = FFNet(arch = architecture, activation = activation_functions[i])\n",
    "    writer_temp = GiottoSummaryWriter(log_dir='runs/' + model_temp.__class__.__name__ + activation_string[i])\n",
    "    trainer_temp = Trainer(model_temp, [dl_tr, dl_ts], loss_function, writer_temp)\n",
    "    models.append(model_temp)\n",
    "    writers.append(writer_temp)\n",
    "    trainers.append(trainer_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train our models!\n",
    "\n",
    "You can monitor the training in the tensorboard page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.616096 \tEpoch training accuracy: 62.94%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berkouknicolas/giotto-deep/gdeep/trainer/trainer.py:455: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " accuracy: 71.81%,                 Avg loss: 0.551802 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.520029 \tEpoch training accuracy: 72.37%                                                \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.96%,                 Avg loss: 0.509592 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.508392 \tEpoch training accuracy: 72.53%                                                \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.91%,                 Avg loss: 0.498214 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.505045 \tEpoch training accuracy: 72.77%                                                \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.84%,                 Avg loss: 0.511900 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.497035 \tEpoch training accuracy: 73.33%                                                 67.0  \t[ 22 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.46%,                 Avg loss: 0.511323 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.492934 \tEpoch training accuracy: 73.40%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.74%,                 Avg loss: 0.511535 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.496636 \tEpoch training accuracy: 73.16%                                                 \t[ 78 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.51%,                 Avg loss: 0.492400 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.497398 \tEpoch training accuracy: 73.29%                                                  ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.05%,                 Avg loss: 0.492634 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.495481 \tEpoch training accuracy: 73.28%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.28%,                 Avg loss: 0.493600 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.496682 \tEpoch training accuracy: 73.29%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.74%,                 Avg loss: 0.496192 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.621385 \tEpoch training accuracy: 60.03%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.09%,                 Avg loss: 0.528542 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.484728 \tEpoch training accuracy: 70.99%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.58%,                 Avg loss: 0.452311 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.448551 \tEpoch training accuracy: 73.57%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.90%,                 Avg loss: 0.465025 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.432312 \tEpoch training accuracy: 74.20%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.66%,                 Avg loss: 0.424690 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.422412 \tEpoch training accuracy: 73.63%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.35%,                 Avg loss: 0.408809 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.424723 \tEpoch training accuracy: 72.50%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.47%,                 Avg loss: 0.414948 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.423944 \tEpoch training accuracy: 73.73%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 74.94%,                 Avg loss: 0.416432 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.427082 \tEpoch training accuracy: 73.90%                                                  \t[ 50 / 160 ]                     72.5  \t[ 104 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.55%,                 Avg loss: 0.413085 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.426498 \tEpoch training accuracy: 72.92%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.62%,                 Avg loss: 0.452170 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.417411 \tEpoch training accuracy: 72.97%                                                 \n",
      "Time taken for this epoch: 2.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.09%,                 Avg loss: 0.413330 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638997 \tEpoch training accuracy: 60.07%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.35%,                 Avg loss: 0.579558 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.579385 \tEpoch training accuracy: 65.30%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.56%,                 Avg loss: 0.568081 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.570656 \tEpoch training accuracy: 64.97%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.36%,                 Avg loss: 0.564179 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.568049 \tEpoch training accuracy: 64.45%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 64.60%,                 Avg loss: 0.557675 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.556032 \tEpoch training accuracy: 64.62%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.95%,                 Avg loss: 0.540214 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.550411 \tEpoch training accuracy: 65.28%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.81%,                 Avg loss: 0.538129 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.542208 \tEpoch training accuracy: 65.82%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.03%,                 Avg loss: 0.528766 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.543355 \tEpoch training accuracy: 66.24%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.26%,                 Avg loss: 0.523859 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.537897 \tEpoch training accuracy: 66.98%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 68.16%,                 Avg loss: 0.520809 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.540835 \tEpoch training accuracy: 66.56%                                                \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.88%,                 Avg loss: 0.524094 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.694774 \tEpoch training accuracy: 50.25%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 56.20%,                 Avg loss: 0.690012 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.647067 \tEpoch training accuracy: 60.03%                                       ]                      56.99999999999999  \t[ 133 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.12%,                 Avg loss: 0.620611 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.614754 \tEpoch training accuracy: 60.88%                                       ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.33%,                 Avg loss: 0.617257 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.613568 \tEpoch training accuracy: 60.92%                                      ]                      \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.35%,                 Avg loss: 0.616549 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.612550 \tEpoch training accuracy: 60.98%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.41%,                 Avg loss: 0.617383 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.605666 \tEpoch training accuracy: 62.57%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 64.66%,                 Avg loss: 0.597559 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.580942 \tEpoch training accuracy: 65.63%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.85%,                 Avg loss: 0.576157 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.572481 \tEpoch training accuracy: 66.04%                                                \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.74%,                 Avg loss: 0.573188 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.570452 \tEpoch training accuracy: 66.23%                                                             65.0  \t[ 110 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.60%,                 Avg loss: 0.570379 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.567547 \tEpoch training accuracy: 66.21%                                                \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.70%,                 Avg loss: 0.568020 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pipe in trainers:\n",
    "    pipe.train(\n",
    "    Adam,\n",
    "    10,\n",
    "    False,\n",
    "    {\"lr\": 0.01},\n",
    "    {\"batch_size\": 200})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each model, let's plot the persistence diagrams of the dataset embedded in each layer of the network\n",
    "\n",
    "To see the persistence diagrams, go to the tensorboard page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.visualisation import Visualiser\n",
    "batch_size = 1000\n",
    "one_batch_dataset, _, _ = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build(\n",
    "    [{\"batch_size\":batch_size, \"sampler\":RandomSampler(ds_tr)}, \n",
    "    {\"batch_size\":batch_size, \"sampler\":RandomSampler(ds_tr)}, \n",
    "    {\"batch_size\":batch_size, \"sampler\":RandomSampler(ds_tr)}]) \n",
    "\n",
    "for pipe in trainers:\n",
    "    vs = Visualiser(pipe)\n",
    "    vs.plot_betti_curves_layers(homology_dimensions = [1], \n",
    "        batch = next(iter(one_batch_dataset)), \n",
    "        k=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for pipe in trainers:\n",
    "    vs = Visualiser(pipe)\n",
    "    vs.plot_persistence_diagrams(next(iter(one_batch_dataset)), k= 0)\n",
    "\n",
    "\n",
    "\n",
    "one_batch_dataset, _, _ = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build([{\"batch_size\":batch_size, \"sampler\":RandomSampler(ds_tr)}, {\"batch_size\":batch_size, \"sampler\":RandomSampler(ds_tr)}, {\"batch_size\":batch_size, \"sampler\":RandomSampler(ds_tr)}]) \n",
    "\n",
    "for pipe in trainers:\n",
    "    vs = Visualiser(pipe)\n",
    "    vs.betti_plot_layers(homology_dimension = [0,1], batch = next(iter(one_batch_dataset)), k=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.analysis.interpretability import Interpreter\n",
    "\n",
    "\n",
    "vs = Visualiser(trainers[0]) \n",
    "vs.plot_3d_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtda.homology import VietorisRipsPersistence\n",
    "\n",
    "data =torch.stack(activations[:-2]).cpu().detach().numpy()\n",
    "VR = VietorisRipsPersistence()\n",
    "VR.fit_transform_plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
