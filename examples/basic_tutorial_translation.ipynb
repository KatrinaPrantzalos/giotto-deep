{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: translation\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functionalities of *giotto-deep* API.\n",
    "\n",
    "The example described in this tutorial is the one of translation. You will build your own transformer model, train it, and use it to translate from German to English!\n",
    "\n",
    "Here is an example of what a translation task is about:\n",
    "\n",
    " - German sentence: 'Ich mag Pizza.'\n",
    " - English translation: 'I like pizza.'\n",
    "\n",
    "## The plan for this tutorial\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. definition of the metrics and losses\n",
    " 4. trainining of the model\n",
    " 5. using the model to translate some sentences\n",
    " 6. (extra) extract some features of the network for further analysis\n",
    " \n",
    "Let's start with importing the requried libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "\n",
    "\n",
    "# our special guests!\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "from gdeep.data.datasets import DatasetBuilder\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.data import TransformingDataset\n",
    "from gdeep.data.preprocessors import TokenizerTranslation\n",
    "from gdeep.data.datasets import DataLoaderBuilder\n",
    "from gdeep.models import ModelExtractor\n",
    "from gdeep.visualisation import Visualiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the results of your models, you need to start tensorboard. All data about the model, the training, the hyperparameters... will be stored there.\n",
    "\n",
    "## How to start tensorboard\n",
    "On the terminal, move inside the `/examples` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "We propose to use the [Multi30k](https://github.com/multi30k/dataset) dataset: in the next cell you will see how easy it is to import datasets in giotto-deep (as easy as iit is in `torchtext`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the builder class\n",
    "bd = DatasetBuilder(name=\"Multi30k\", convert_to_map_dataset=True)\n",
    "\n",
    "# build the dataset\n",
    "ds_tr_str, ds_val_str, ds_ts_str = bd.build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a list of pairs of sentences: the German sentence and its English translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before preprocessing: \\n\", ds_tr_str[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required preprocessing\n",
    "\n",
    "Neural networks cannot direcly deal with strings. We have first to preprocess the dataset in three main ways:\n",
    " 1. Tokenise each string into its words (and maybe adjust each word to remove plurals, interjections, capital letters...)\n",
    " 2. Build a vocabulary out of these tokens (each word so modified is called token)\n",
    " 3. Embed each token into a vector, so that each sentence becomes a list of vectors\n",
    "\n",
    "The **first two steps** are performed by the `TokenizerTranslation` class. The embedding will be added directly to the model (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# install german tokenizer\n",
    "!{sys.executable} -m spacy download de_core_news_sm\n",
    "\n",
    "# get the german tokenizer\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "# initialise the giotto-deep tokenizer\n",
    "tokenizer = TokenizerTranslation(tokenizer=de_tokenizer)\n",
    "\n",
    "# fit the tokenizer to the dataset (note that te vocabularies will be automatically built in this case\n",
    "tokenizer.fit_to_dataset(ds_tr_str)\n",
    "\n",
    "# prprocess the dataset\n",
    "transformed_textds = tokenizer.attach_transform_to_dataset(ds_tr_str)\n",
    "transformed_textts = tokenizer.attach_transform_to_dataset(ds_val_str)\n",
    "\n",
    "print(\"After the preprocessing: \\n\", transformed_textds[0])\n",
    "\n",
    "# subsample the training and test datasets\n",
    "train_indices = list(range(64*2))\n",
    "test_indices = list(range(64*1))\n",
    "\n",
    "dl_tr, dl_val, _ = DataLoaderBuilder((transformed_textds, \n",
    "                                   transformed_textts)).build(({\"batch_size\":16, \n",
    "                                                                          \"sampler\":SubsetRandomSampler(train_indices)},{\"batch_size\":16, \n",
    "                                                                          \"sampler\":SubsetRandomSampler(test_indices)}\n",
    "                                                                          ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now the data is not in string format anymore: rather, the sentences have each been transformed to a `torch.Tensor` of type `long`. Each of these umbers represents the index in the vocabulary of the associated token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your model\n",
    "\n",
    "The model we play with is a simple transformer model with two embedding layers (for the German and English sentence) followed by a single transformer layer. \n",
    "\n",
    "The input ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# my simple transformer model\n",
    "class TranslationTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim):\n",
    "        super(TranslationTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=embed_dim,\n",
    "                                       nhead=2,\n",
    "                                       num_encoder_layers=1,\n",
    "                                       num_decoder_layers=1,\n",
    "                                       dim_feedforward=512,\n",
    "                                       dropout=0.1)\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, embed_dim, sparse=True)\n",
    "        self.generator = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        src = X[:,0,:]\n",
    "        tgt = X[:,1,:]\n",
    "        src_emb = self.embedding_src(src)\n",
    "        tgt_emb = self.embedding_tgt(tgt)\n",
    "        self.outs = self.transformer(src_emb, tgt_emb)\n",
    "        logits = self.generator(self.outs)\n",
    "        return logits\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"this method is used only at the inference step.\n",
    "        This method runs the data through the encoder of the\n",
    "        transformer\"\"\"\n",
    "        return self.transformer.encoder(\n",
    "                            self.embedding_src(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        \"\"\"this method is used only at the inference step\n",
    "        This method runs the data through the decoder of the\n",
    "        transformer\"\"\"\n",
    "        return self.transformer.decoder(\n",
    "                          self.embedding_tgt(tgt), memory,\n",
    "                          tgt_mask)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise the model as follows:\n",
    " - we need to set the maximum vocabulary size to fix the `Embedding` architectures\n",
    " - we need to set the embedding dimension\n",
    " - initialise the model class with the needed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000 # to be discussed\n",
    "\n",
    "src_vocab_size = vocab_size # len(prec.vocabulary)\n",
    "tgt_vocab_size = vocab_size # len(prec.vocabulary_target)\n",
    "emb_dim = 64\n",
    "\n",
    "model = TranslationTransformer(src_vocab_size, tgt_vocab_size, emb_dim)\n",
    "X = next(iter(dl_tr));  # a datum\n",
    "#assert model(X[0]).argmax(2).shape == X[1].shape\n",
    "print(\"This is our model: \\n\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "This loss function is an adapted version of the Cross Entropy Loss for the transformer architecture we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(logits, tgt_out):\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    return cel(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we are: all is set upo and we are ready to train the model.\n",
    "\n",
    "# Trainig the model\n",
    "\n",
    "In giotto deep all is done via a `Trainer`, meaning a class taht takes care of the training and validation steps, storing the intermediate results to tensorboard, using (or not) cross validation, ... basically everythin you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a pipeline class with the model, dataloaders loss_fn and tensorboard writer\n",
    "pipe = Trainer(model, (dl_tr, dl_val), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(optimizer=SGD, \n",
    "           n_epochs=3, \n",
    "           cross_validation=False, \n",
    "           optimizers_param={\"lr\":0.01}, \n",
    "           dataloaders_param={\"batch_size\":16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation!\n",
    "\n",
    "So the model has been trained and now it is time to translate a sentece. Let's take thee following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de, en = next(iter(ds_tr_str))\n",
    "print(de, \"\\n\", en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get the vocabulary and numericize the German sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = tokenizer.vocabulary\n",
    "#sent = str.lower(de).split()\n",
    "#de_sentence = list(map(voc.__getitem__,sent))\n",
    "de_sentence = tokenizer((de, en))[0][0]\n",
    "de_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build a couple of auxiliary functiosn to help us translate. The logic is to use the transformer encoder and decoder layers directly. Let's have a quick reminder o how things work:\n",
    "\n",
    "<img src=\"./images/translation_transformer.png\" alt=\"drawing\" width=\"400\" class=\"center\"/>\n",
    "\n",
    "The above drawing is a good representation of our architecture. During training, the English and German sentence are inputted from \"below\". Then, once the model is used in inference, there is no need to use the \"Outputs\", as the \"Input\" ncoder will have its information flow though the transfomer till he probabilistic output on top: this is where we get the translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"function to generate output sequence using greedy algorithm\"\"\"\n",
    "    memory = model.encode(src, src_mask)\n",
    "    out = model.decode(src, memory, None)\n",
    "    prob = model.generator(out)\n",
    "    greedy_out = torch.max(prob, dim=2).indices\n",
    "    return greedy_out\n",
    "    \n",
    "\n",
    "def translate(model: torch.nn.Module, dl_ts_item):\n",
    "    \"\"\"actual function to translate input sentence into target language\"\"\"\n",
    "    model.eval()\n",
    "    voc = tokenizer.vocabulary_target\n",
    "    src = dl_ts_item\n",
    "    num_tokens = src.shape[1]\n",
    "    src_mask = None\n",
    "    tgt_tokens_raw = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=111).flatten()\n",
    "    tgt_tokens = [token for token in tgt_tokens_raw if token < len(list(voc.vocab))]\n",
    "    return \" \".join(list(map(list(voc.vocab).__getitem__, tgt_tokens)))\n",
    "    \n",
    "# translation!\n",
    "print(\"German sentence: \", de)\n",
    "print(\"English translation: \", translate(pipe.model, de_sentence.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data saliency map on translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have just run this notebooks, you would most likely have gotten some funny nonsensical answer: consider that you trained a very simple model on a very small subset of data for only two epochs.\n",
    "\n",
    "## Challenge\n",
    "\n",
    "Starting from this simple notebook, do you think you can enlarge the dataset, the model, the training epochs and get a decent translator? Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface transformers\n",
    "\n",
    "If you are actually interested in a pretrained transformers that works directly with a few lines of code, `giotto-deep` supports Hugginface transformers.\n",
    "\n",
    "The next section explains how to run one such transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ...\n",
    "\n",
    "# take T5small\n",
    "\n",
    "# finetune on the Multi30k\n",
    "\n",
    "# go to TB to see the results + screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Extract inner data from your models\n",
    "\n",
    "With `gdeep` is is pretty straight forwasrd to extract data about the inner working of the models.\n",
    "\n",
    "For example you can use the `ModelExtract` and get:\n",
    " - the parameters of each layer\n",
    " - the activation functions (given an input)\n",
    " - the gradients, given a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialise the ModelExtractor\n",
    "me = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k,item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "x = next(iter(dl_tr))[0]\n",
    "pipe.model.eval()\n",
    "pipe.model(x.to(DEVICE))\n",
    "\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, target = next(iter(dl_tr))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualise the model graph on tensorboard interactively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialise the Visualiser\n",
    "vs = Visualiser(pipe)\n",
    "\n",
    "vs.plot_data_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
