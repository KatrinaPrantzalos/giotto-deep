{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toplogy of Deep Neural Networks\n",
    "\n",
    "This notebook will show you how easy it is to use gdeep to reproduce the experiments of the paper [Topology of Deep Neural Networks](https://arxiv.org/pdf/2004.06093.pdf), by Naizat et. al. In this work, the authors studied the evolution of the topology of a dataset as embedded in the successive layers of a Neural Network, trained for classification on this dataset.\n",
    "\n",
    "Their main findings can be summarized as follows:\n",
    "\n",
    "- Neural networks tend to simplify the topology of the dataset accross layers.\n",
    "\n",
    "- This decrease in topological complexity is more efficient when the activation functions are non-homeomorphic, as it is the case for ReLu or leakyReLu.\n",
    "\n",
    "Here is an illustration from the paper:\n",
    "\n",
    "![img](./images/topology_accross_layers.png)\n",
    "\n",
    "The main steps of this tutorial will be as follows:\n",
    "\n",
    "1. Create the Entangled Tori dataset.\n",
    "2. Build several fully connected networks, with different activation functions.\n",
    "3. Train these networks to classify the Entangled Tori datasets.\n",
    "4. Visualise in tensorboard the persistence diagrams of the dataset embedded in each layers of each network.\n",
    "5. Study the decrease in topological complexity of the dataset accross layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import autograd  \n",
    "\n",
    "#gdeep\n",
    "from gdeep.data.datasets import DatasetBuilder, DataLoaderBuilder\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualisation import persistence_diagrams_of_activations\n",
    "from gdeep.data.preprocessors import ToTensorImage\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.search import Benchmark\n",
    "from gdeep.search import GiottoSummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "# plot\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "writer = GiottoSummaryWriter()\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# TDA\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "#Tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the Entangled Tori dataset and prepare the dataloaders\n",
    "\n",
    "![img](./images/entangled_tori.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  RandomSampler\n",
    "db = DatasetBuilder(name=\"EntangledTori\")\n",
    "ds_tr, ds_val, ds_ts = db.build( n_pts = 50)\n",
    "dl_tr, dl_val, dl_ts = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build(    \n",
    "     [{\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}, \n",
    "     {\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}, \n",
    "     {\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models with different activations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Choose the achitecture of the fully connected network\n",
    "architecture = [3,5,5,5,2]\n",
    "# Choose the loss function for training\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Choose the set of activation functions to equip the neural network with\n",
    "activation_string = [\"relu\", \"leakyrelu\", \"tanh\", \"sigmoid\"]\n",
    "activation_functions = [F.relu, F.leaky_relu, torch.tanh, torch.sigmoid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and trainers\n",
    "models = []\n",
    "writers = []\n",
    "trainers = []\n",
    "for i in range(len(activation_functions)):\n",
    "    model_temp = FFNet(arch = architecture, activation = activation_functions[i])\n",
    "    writer_temp = GiottoSummaryWriter(log_dir='runs/' + model_temp.__class__.__name__ + activation_string[i])\n",
    "    trainer_temp = Trainer(model_temp, [dl_tr, dl_ts], loss_function, writer_temp)\n",
    "    models.append(model_temp)\n",
    "    writers.append(writer_temp)\n",
    "    trainers.append(trainer_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train our models!\n",
    "\n",
    "You can monitor the training in the tensorboard page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.619556 \tEpoch training accuracy: 57.38%                                                             / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berkouknicolas/giotto-deep/gdeep/trainer/trainer.py:455: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " accuracy: 63.54%,                 Avg loss: 0.569001 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.557137 \tEpoch training accuracy: 63.68%                                                             8.5  \t[ 59 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.28%,                 Avg loss: 0.544553 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.533706 \tEpoch training accuracy: 66.52%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.25%,                 Avg loss: 0.535445 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.518293 \tEpoch training accuracy: 67.78%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.42%,                 Avg loss: 0.515521 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.504022 \tEpoch training accuracy: 68.32%                                                71.5  \t[ 119 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.69%,                 Avg loss: 0.506407 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.499697 \tEpoch training accuracy: 68.92%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.97%,                 Avg loss: 0.498756 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.494252 \tEpoch training accuracy: 69.47%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.24%,                 Avg loss: 0.492811 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.487563 \tEpoch training accuracy: 70.31%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.81%,                 Avg loss: 0.490700 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.486185 \tEpoch training accuracy: 70.62%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.78%,                 Avg loss: 0.477171 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.481709 \tEpoch training accuracy: 70.92%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 69.79%,                 Avg loss: 0.483383 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.617858 \tEpoch training accuracy: 60.37%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.34%,                 Avg loss: 0.545143 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.526118 \tEpoch training accuracy: 69.05%                                                                               \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.19%,                 Avg loss: 0.494957 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.484463 \tEpoch training accuracy: 71.36%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.20%,                 Avg loss: 0.486658 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.476393 \tEpoch training accuracy: 72.71%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 74.35%,                 Avg loss: 0.468885 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.465183 \tEpoch training accuracy: 73.45%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.51%,                 Avg loss: 0.456342 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.454584 \tEpoch training accuracy: 74.54%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.46%,                 Avg loss: 0.443893 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.450285 \tEpoch training accuracy: 74.60%                                                 76.5  \t[ 27 / 160 ]                      72.0  \t[ 121 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 76.14%,                 Avg loss: 0.444930 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.448743 \tEpoch training accuracy: 75.15%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 76.11%,                 Avg loss: 0.453317 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.448261 \tEpoch training accuracy: 75.45%                                                 86 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.30%,                 Avg loss: 0.450963 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.445369 \tEpoch training accuracy: 75.67%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 76.19%,                 Avg loss: 0.444567 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.608111 \tEpoch training accuracy: 62.35%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.33%,                 Avg loss: 0.530383 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.508557 \tEpoch training accuracy: 73.58%                                                 74.0  \t[ 121 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 74.39%,                 Avg loss: 0.497576 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.490307 \tEpoch training accuracy: 74.23%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 74.65%,                 Avg loss: 0.481984 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.480013 \tEpoch training accuracy: 74.46%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 74.59%,                 Avg loss: 0.472669 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.478102 \tEpoch training accuracy: 74.08%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.65%,                 Avg loss: 0.476270 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.476157 \tEpoch training accuracy: 74.46%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.29%,                 Avg loss: 0.465043 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.470918 \tEpoch training accuracy: 74.62%                                                 5.0  \t[ 63 / 160 ]                     72.5  \t[ 125 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.55%,                 Avg loss: 0.467201 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.471308 \tEpoch training accuracy: 74.53%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.34%,                 Avg loss: 0.462861 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.467190 \tEpoch training accuracy: 74.52%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.12%,                 Avg loss: 0.469554 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.467016 \tEpoch training accuracy: 74.68%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.80%,                 Avg loss: 0.461124 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.678610 \tEpoch training accuracy: 55.50%                                                              61.5  \t[ 160 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.42%,                 Avg loss: 0.635621 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.624147 \tEpoch training accuracy: 60.35%                                       ]                     28 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.77%,                 Avg loss: 0.619441 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.616846 \tEpoch training accuracy: 60.68%                                       ]                      \t[ 57 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.74%,                 Avg loss: 0.617125 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.612746 \tEpoch training accuracy: 61.67%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.79%,                 Avg loss: 0.608781 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.591547 \tEpoch training accuracy: 65.53%                                                             \t[ 60 / 160 ]                      / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.36%,                 Avg loss: 0.574867 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.572152 \tEpoch training accuracy: 66.37%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.12%,                 Avg loss: 0.565294 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.566669 \tEpoch training accuracy: 66.92%                                                             71.0  \t[ 58 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.99%,                 Avg loss: 0.565564 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.564476 \tEpoch training accuracy: 66.82%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.51%,                 Avg loss: 0.562308 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.564062 \tEpoch training accuracy: 67.01%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.70%,                 Avg loss: 0.559171 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.563189 \tEpoch training accuracy: 67.12%                                                \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.61%,                 Avg loss: 0.560641 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pipe in trainers:\n",
    "    pipe.train(\n",
    "    Adam,\n",
    "    10,\n",
    "    False,\n",
    "    {\"lr\": 0.01},\n",
    "    {\"batch_size\": 200})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each model, let's plot the topology of the dataset embedded in each layer of the network\n",
    "\n",
    "We start by the Betti curves. For a subset of size batch_size of the dataset, we compute the successive Betti numbers of the Vietoris-Rips complex of radius filtration_value of the subset embedded in each layer of the network. The result is plotted in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.visualisation import Visualiser\n",
    "#Choose the size of the subset of the dataset\n",
    "batch_size = 1000\n",
    "#Extract the subset of the dataset\n",
    "one_batch_dataset, _, _ = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build(\n",
    "    [{\"batch_size\":batch_size, \"sampler\":RandomSampler(ds_tr)}, \n",
    "    {\"batch_size\":batch_size, \"sampler\":RandomSampler(ds_tr)}, \n",
    "    {\"batch_size\":batch_size, \"sampler\":RandomSampler(ds_tr)}]) \n",
    "\n",
    "#For each model, plot the Betti curve\n",
    "for pipe in trainers:\n",
    "    vs = Visualiser(pipe)\n",
    "    vs.plot_betti_numbers_layers(homology_dimensions = [0,1], \n",
    "        batch = next(iter(one_batch_dataset)), \n",
    "        filtration_value = 0.5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a bit more time, you can even compute the Persistence Diagrams of the subset of the dataset embedded in each layers, and plot them in tensorboard! The computation might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pipe in trainers:\n",
    "    vs = Visualiser(pipe)\n",
    "    vs.plot_persistence_diagrams(next(iter(one_batch_dataset)), k= 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
