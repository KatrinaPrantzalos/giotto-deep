{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# This snippet will deactivate autoreload if this file\n","# is run as a script.\n","from gdeep.utility.utils import is_notebook\n","\n","if is_notebook:\n","    # Autoreload modules\n","    from IPython import get_ipython  # type: ignore\n","    get_ipython().magic('load_ext autoreload')\n","    get_ipython().magic('autoreload 2')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import os\n","from os import remove\n","from os.path import join\n","\n","from gdeep.data import DlBuilderFromDataCloud, DatasetCloud\n","from gdeep.utility.utils import get_checksum\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["folder = \"examples/data/DatasetCloud/SmallDataset\"\n","# Compute checksums for all files folder of type 'pt' and print them\n","def print_checksums(folder):\n","    \"\"\"Prints the checksums of all files in folder of type 'pt'.\n","    \n","    Parameters\n","    ----------\n","    folder : str\n","        The path to the folder.\n","    \n","    Returns\n","    -------\n","    None\n","        This function does not return anything.\n","    \"\"\"\n","    for file in os.listdir(folder):\n","        if file.endswith(\".pt\"):\n","            print(file, \":\", get_checksum(join(folder, file)))\n","\n","print_checksums(folder)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Using the Dataset Cloud to train topological models\n"," In this tutorial we will use the our custom datasets storage on\n"," [Google Cloud Datastore](https://cloud.google.com/datastore/) to\n"," to store and load our datasets.\n"," The dataset cloud storage contain a variety of topological datasets that\n"," can be easily used in GDeep."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# To see all publically available datasets, please use the following command:\n","DatasetCloud(\"\").get_existing_dataset()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Uploading and downloading datasets from the cloud\n"," Using datasets from the cloud is very easy. The datasets are publicly\n"," available and can be downloaded from the cloud without any registration.\n","\n"," To upload a dataset to the cloud, you have to have a Google Cloud API key\n"," to the Google Cloud Datastore bucket. If you are interested uploading your\n"," own dataset, please contact us at\n"," [raphael.reinauer@epfl.ch](mailto:raphael.reinauer@epfl.ch)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_and_upload_dataset():\n","    \"\"\"The method above creates a dataset with random data and labels,\n","    saves it locally as pickled files, and then uploads it to the Cloud.\n","    The dataset is then deleted from the local machine.\n","    \n","    Returns\n","    -------\n","    None\n","        This function does not return anything.\n","    \"\"\"\n","    # Generate a dataset\n","    # You do not have to do that if you already have a pickled dataset\n","    size_dataset = 100\n","    input_dim = 5\n","    num_labels = 2\n","    data = torch.rand(size_dataset, input_dim)\n","    labels = torch.randint(0, num_labels, (size_dataset,)).long()\n","\n","    # pickle data and labels\n","    data_filename = 'tmp_data.pt'\n","    labels_filename = 'tmp_labels.pt'\n","    torch.save(data, data_filename)\n","    torch.save(labels, labels_filename)\n","\n","    ## Upload dataset to Cloud\n","    dataset_name = \"SmallDataset2\"\n","    dataset_cloud = DatasetCloud(dataset_name)\n","\n","    # Specify the metadata of the dataset\n","    dataset_cloud._add_metadata(\n","        name=dataset_name,\n","        input_size=(input_dim,),\n","        size_dataset=size_dataset,\n","        num_labels=num_labels,\n","        data_type=\"tabular\",\n","        data_format=\"pytorch_tensor\",\n","    )\n","\n","    # upload dataset to Cloud\n","    dataset_cloud._upload(data_filename, labels_filename)\n","\n","    # remove the labels and data files\n","    # Warning: Only do this if you do want the local dataset to be deleted!\n","    remove(data_filename)\n","    remove(labels_filename)\n","    \n","create_and_upload_dataset()"]},{"cell_type":"markdown","metadata":{},"source":[" ## Using the Dataset Cloud to train topological model\n"," The datasets in the cloud are automatically downloaded and used by GDeep.\n"," Only specify the dataset name and the path you want to save the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create dataloaders from data cloud\n","# If you don't know what datasets exist in the cloud, just use an empty\n","# ´dataset_name´ and then the error message will display all available datasets \n","dataset_name = \"MutagDataset\"\n","download_directory = join(\"data\", \"DatasetCloud\")\n","\n","dl_cloud_builder = DlBuilderFromDataCloud(dataset_name,\n","                                   download_directory)\n","\n","# You can display the metadata of the dataset\n","print(dl_cloud_builder.get_metadata())\n","\n","# create the dataset from the downloaded dataset\n","train_dataloader, val_dataloader, test_dataloader = \\\n","    dl_cloud_builder.build_dataloaders(batch_size=10)\n","\n","del train_dataloader, val_dataloader, test_dataloader\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Now you can train a model on the dataset using the created dataloaders."]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}