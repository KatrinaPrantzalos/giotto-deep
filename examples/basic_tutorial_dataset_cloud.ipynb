{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from gdeep.utility.utils import is_notebook\n","\n","if is_notebook:\n","    # Autoreload modules\n","    from IPython import get_ipython  # type: ignore\n","    get_ipython().magic('load_ext autoreload')\n","    get_ipython().magic('autoreload 2')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import os\n","from os import remove\n","from os.path import join\n","\n","from gdeep.data import DlBuilderFromDataCloud, DatasetCloud\n","\n","from gdeep.search import PersformerHyperparameterSearch"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def create_and_upload_dataset():\n","    \"\"\"The method above creates a dataset with random data and labels,\n","    saves it locally as pickled files, and then uploads it to the Cloud.\n","    The dataset is then deleted from the local machine.\n","    \n","    Returns\n","    -------\n","    None\n","        This function does not return anything.\n","    \"\"\"\n","    # Generate a dataset\n","    # You do not have to do that if you already have a pickled dataset\n","    size_dataset = 100\n","    input_dim = 5\n","    num_labels = 2\n","    data = torch.rand(size_dataset, input_dim)\n","    labels = torch.randint(0, num_labels, (size_dataset,)).long()\n","\n","    # pickle data and labels\n","    data_filename = 'tmp_data.pt'\n","    labels_filename = 'tmp_labels.pt'\n","    torch.save(data, data_filename)\n","    torch.save(labels, labels_filename)\n","\n","    ## Upload dataset to Cloud\n","    dataset_name = \"SmallDataset2\"\n","    dataset_cloud = DatasetCloud(dataset_name)\n","\n","    # Specify the metadata of the dataset\n","    dataset_cloud._add_metadata(\n","        name=dataset_name,\n","        input_size=(input_dim,),\n","        size_dataset=size_dataset,\n","        num_labels=num_labels,\n","        data_type=\"tabular\",\n","        data_format=\"pytorch_tensor\",\n","    )\n","\n","    # upload dataset to Cloud\n","    dataset_cloud._upload(data_filename, labels_filename)\n","\n","    # remove the labels and data files\n","    # Warning: Only do this if you do want the local dataset to be deleted!\n","    remove(data_filename)\n","    remove(labels_filename)\n","    \n","create_and_upload_dataset()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create dataloaders from data cloud\n","# If you don't know what datasets exist in the cloud, just use an empty\n","# ´dataset_name´ and then the error message will display all available datasets \n","dataset_name = \"MutagDataset\"\n","download_directory = join(\"data\", \"DatasetCloud\")\n","\n","dl_cloud_builder = DlBuilderFromDataCloud(dataset_name,\n","                                   download_directory)\n","\n","# You can display the metadata of the dataset\n","print(dl_cloud_builder.get_metadata())\n","\n","# create the dataset from the downloaded dataset\n","train_dataloader, val_dataloader, test_dataloader = dl_cloud_builder.build_dataloaders(batch_size=10)\n","\n","del train_dataloader, val_dataloader, test_dataloader\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}